{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English Hindi Translation Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wqO4ynZ4P-cO",
        "colab_type": "code",
        "outputId": "c9865b40-ccc0-4760-c3c5-50c2e8b2f979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QhhM5RWjQw8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# English - Hindi Translation Chatbot\n"
      ]
    },
    {
      "metadata": {
        "id": "xmw2wcYsQtlb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "DkzR9gIhRFy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "#os.chdir('E:/Anupam/HomeDocs/DataScience/ExternalAssignments/NLP/Language Translation/')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBcNHVTRSAPG",
        "colab_type": "code",
        "outputId": "86bddc1d-85a0-4cd9-eab1-4a29b29deaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#Change the working directory\n",
        "os.chdir('/content/gdrive/My Drive/Language Translation')\n",
        "os.listdir()\n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dataset',\n",
              " 'seq2seq_encoder_eng_hin.hd5',\n",
              " 'seq2seq_decoder_eng_hin.hd5',\n",
              " 'encoder_tokenizer_eng',\n",
              " 'decoder_tokenizer_hin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "metadata": {
        "id": "ABc05yuHRKXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk\n",
        "import re \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as backend\n",
        "import codecs\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdNXPqz-RliM",
        "colab_type": "code",
        "outputId": "c26c138c-254f-45b0-b716-edb6343bb9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "with codecs.open('Dataset/Engligh-Hindi Translation Dataset.txt', encoding='utf-8') as f:\n",
        "    texual_data = f.read()\n",
        "texual_data[0:200]"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ufeffWow!\\tवाह!\\r\\nHelp!\\tबचाओ!\\r\\nJump.\\tउछलो.\\r\\nJump.\\tकूदो.\\r\\nJump.\\tछलांग.\\r\\nHello!\\tनमस्ते।\\r\\nHello!\\tनमस्कार।\\r\\nCheers!\\tवाह-वाह!\\r\\nCheers!\\tचियर्स!\\r\\nGot it?\\tसमझे कि नहीं?\\r\\nI'm OK.\\tमैं ठीक हूँ।\\r\\nAwesome!\\tबहुत बढ़िया!\\r\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "metadata": {
        "id": "IIpgbQhLSV9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Spell Correction, Removing punctuation, Preparing word to vector dictionary"
      ]
    },
    {
      "metadata": {
        "id": "MEuNH9FuSKjY",
        "colab_type": "code",
        "outputId": "39e4130c-9932-4300-b443-9f92ccd713cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#Split by newline character\n",
        "texual_data =  texual_data.split('\\n')\n",
        "#Show some Data\n",
        "texual_data[100:105]"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I don't know.\\tमुझे नहीं मालूम।\\r\",\n",
              " 'I have a car.\\tमेरे पास एक गाड़ी है।\\r',\n",
              " 'I have a dog.\\tमेरे पास एक कुत्ता है।\\r',\n",
              " 'I understand.\\tमैं समझता हूँ।\\r',\n",
              " \"I'm a doctor.\\tमैं डॉक्टर हूँ।\\r\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "metadata": {
        "id": "phB_vZ5lSfB2",
        "colab_type": "code",
        "outputId": "b460a97d-06d7-441c-9b88-e34cbd5dfc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(texual_data)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "metadata": {
        "id": "QKHrahxYSjhp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Separate Source and Target pairs"
      ]
    },
    {
      "metadata": {
        "id": "wbgbWjn9Se0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_text = [] #Initialize Source language list\n",
        "decoder_text = [] #Initialize Target language list\n",
        "\n",
        "#Iterate over data\n",
        "for line in texual_data:\n",
        "    try:\n",
        "        english_text, hindi_text = line.split('\\t')\n",
        "        encoder_text.append(english_text)\n",
        "        \n",
        "        # Add tab '<start>' as 'start sequence in target\n",
        "        # And '<end>' as End\n",
        "        hindi_text = hindi_text.replace('\\r', '')\n",
        "        decoder_text.append('<start> ' + hindi_text + ' <end>')\n",
        "    except:\n",
        "        pass #ignore data which goes into error  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TO8Y93vSrjH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Separate Source and Target pairs"
      ]
    },
    {
      "metadata": {
        "id": "qE-3hp4GSeeA",
        "colab_type": "code",
        "outputId": "eaa87647-18fc-46ff-9d7e-31a8868b3a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "display(encoder_text[100:105])\n",
        "decoder_text[100:105]"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\"I don't know.\",\n",
              " 'I have a car.',\n",
              " 'I have a dog.',\n",
              " 'I understand.',\n",
              " \"I'm a doctor.\"]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> मुझे नहीं मालूम। <end>',\n",
              " '<start> मेरे पास एक गाड़ी है। <end>',\n",
              " '<start> मेरे पास एक कुत्ता है। <end>',\n",
              " '<start> मैं समझता हूँ। <end>',\n",
              " '<start> मैं डॉक्टर हूँ। <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "metadata": {
        "id": "oiGj1AiuSyra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize Source language sentences"
      ]
    },
    {
      "metadata": {
        "id": "rAwVNK_bS0cE",
        "colab_type": "code",
        "outputId": "59081f77-e22d-4c32-fe60-96cd51e6bacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Tokenizer for source language\n",
        "encoder_t = tf.keras.preprocessing.text.Tokenizer()\n",
        "encoder_t.fit_on_texts(encoder_text) #Fit it on Source sentences\n",
        "encoder_seq = encoder_t.texts_to_sequences(encoder_text) #Convert sentences to numbers \n",
        "encoder_seq[100:105] #Display some converted sentences"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 28, 43], [2, 12, 6, 100], [2, 12, 6, 130], [2, 213], [38, 6, 153]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "metadata": {
        "id": "i_3_31gbS5VP",
        "colab_type": "code",
        "outputId": "eb2740c8-551e-451e-ef04-0cf3f49bdf00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_t.word_index"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'i': 2,\n",
              " 'to': 3,\n",
              " 'you': 4,\n",
              " 'is': 5,\n",
              " 'a': 6,\n",
              " 'he': 7,\n",
              " 'of': 8,\n",
              " 'in': 9,\n",
              " 'my': 10,\n",
              " 'it': 11,\n",
              " 'have': 12,\n",
              " 'this': 13,\n",
              " 'me': 14,\n",
              " 'she': 15,\n",
              " 'for': 16,\n",
              " 'was': 17,\n",
              " 'are': 18,\n",
              " 'do': 19,\n",
              " 'that': 20,\n",
              " 'his': 21,\n",
              " 'your': 22,\n",
              " 'we': 23,\n",
              " 'will': 24,\n",
              " 'what': 25,\n",
              " 'on': 26,\n",
              " 'him': 27,\n",
              " \"don't\": 28,\n",
              " 'at': 29,\n",
              " 'not': 30,\n",
              " 'her': 31,\n",
              " 'like': 32,\n",
              " 'go': 33,\n",
              " 'with': 34,\n",
              " 'be': 35,\n",
              " 'how': 36,\n",
              " 'and': 37,\n",
              " \"i'm\": 38,\n",
              " 'can': 39,\n",
              " 'time': 40,\n",
              " 'there': 41,\n",
              " 'has': 42,\n",
              " 'know': 43,\n",
              " 'all': 44,\n",
              " 'up': 45,\n",
              " 'they': 46,\n",
              " 'come': 47,\n",
              " 'as': 48,\n",
              " 'very': 49,\n",
              " 'had': 50,\n",
              " 'from': 51,\n",
              " \"it's\": 52,\n",
              " 'please': 53,\n",
              " 'did': 54,\n",
              " 'when': 55,\n",
              " 'by': 56,\n",
              " 'want': 57,\n",
              " 'am': 58,\n",
              " 'here': 59,\n",
              " 'out': 60,\n",
              " 'an': 61,\n",
              " 'going': 62,\n",
              " 'been': 63,\n",
              " 'get': 64,\n",
              " 'take': 65,\n",
              " 'about': 66,\n",
              " 'no': 67,\n",
              " 'father': 68,\n",
              " \"can't\": 69,\n",
              " 'book': 70,\n",
              " 'if': 71,\n",
              " 'india': 72,\n",
              " 'were': 73,\n",
              " 'one': 74,\n",
              " 'tom': 75,\n",
              " 'money': 76,\n",
              " 'english': 77,\n",
              " 'two': 78,\n",
              " 'tomorrow': 79,\n",
              " 'would': 80,\n",
              " \"i'll\": 81,\n",
              " 'day': 82,\n",
              " 'long': 83,\n",
              " 'see': 84,\n",
              " 'today': 85,\n",
              " 'where': 86,\n",
              " 'but': 87,\n",
              " 'must': 88,\n",
              " 'back': 89,\n",
              " 'help': 90,\n",
              " 'who': 91,\n",
              " 'make': 92,\n",
              " 'man': 93,\n",
              " 'live': 94,\n",
              " 'us': 95,\n",
              " 'our': 96,\n",
              " 'work': 97,\n",
              " 'good': 98,\n",
              " 'home': 99,\n",
              " 'car': 100,\n",
              " 'now': 101,\n",
              " \"didn't\": 102,\n",
              " 'yesterday': 103,\n",
              " 'these': 104,\n",
              " 'speak': 105,\n",
              " 'should': 106,\n",
              " 'too': 107,\n",
              " 'made': 108,\n",
              " 'so': 109,\n",
              " 'last': 110,\n",
              " 'than': 111,\n",
              " 'many': 112,\n",
              " 'room': 113,\n",
              " 'every': 114,\n",
              " 'never': 115,\n",
              " 'house': 116,\n",
              " 'more': 117,\n",
              " 'nothing': 118,\n",
              " 'mother': 119,\n",
              " 'tell': 120,\n",
              " 'went': 121,\n",
              " 'always': 122,\n",
              " 'much': 123,\n",
              " 'off': 124,\n",
              " 'some': 125,\n",
              " 'old': 126,\n",
              " 'why': 127,\n",
              " 'got': 128,\n",
              " 'again': 129,\n",
              " 'dog': 130,\n",
              " 'down': 131,\n",
              " 'may': 132,\n",
              " 'think': 133,\n",
              " 'could': 134,\n",
              " 'any': 135,\n",
              " 'people': 136,\n",
              " 'door': 137,\n",
              " 'call': 138,\n",
              " 'turn': 139,\n",
              " 'well': 140,\n",
              " 'them': 141,\n",
              " 'let': 142,\n",
              " 'train': 143,\n",
              " \"i've\": 144,\n",
              " 'children': 145,\n",
              " 'give': 146,\n",
              " 'put': 147,\n",
              " 'into': 148,\n",
              " 'school': 149,\n",
              " 'before': 150,\n",
              " 'or': 151,\n",
              " 'away': 152,\n",
              " 'doctor': 153,\n",
              " 'leave': 154,\n",
              " 'lot': 155,\n",
              " 'next': 156,\n",
              " 'teacher': 157,\n",
              " 'say': 158,\n",
              " 'tired': 159,\n",
              " 'new': 160,\n",
              " 'happy': 161,\n",
              " 'night': 162,\n",
              " 'ten': 163,\n",
              " 'water': 164,\n",
              " 'seen': 165,\n",
              " 'answer': 166,\n",
              " 'once': 167,\n",
              " 'left': 168,\n",
              " 'open': 169,\n",
              " 'read': 170,\n",
              " 'hard': 171,\n",
              " 'saw': 172,\n",
              " 'believe': 173,\n",
              " 'use': 174,\n",
              " 'soon': 175,\n",
              " 'keep': 176,\n",
              " 'years': 177,\n",
              " 'way': 178,\n",
              " 'letter': 179,\n",
              " 'because': 180,\n",
              " 'their': 181,\n",
              " \"let's\": 182,\n",
              " 'year': 183,\n",
              " \"isn't\": 184,\n",
              " 'problem': 185,\n",
              " 'japan': 186,\n",
              " 'job': 187,\n",
              " 'watch': 188,\n",
              " 'true': 189,\n",
              " 'does': 190,\n",
              " 'few': 191,\n",
              " 'mary': 192,\n",
              " 'cannot': 193,\n",
              " 'both': 194,\n",
              " 'big': 195,\n",
              " 'drive': 196,\n",
              " 'birthday': 197,\n",
              " 'feel': 198,\n",
              " \"that's\": 199,\n",
              " 'cut': 200,\n",
              " 'met': 201,\n",
              " 'books': 202,\n",
              " 'doing': 203,\n",
              " 'tea': 204,\n",
              " \"couldn't\": 205,\n",
              " 'without': 206,\n",
              " 'month': 207,\n",
              " 'morning': 208,\n",
              " 'try': 209,\n",
              " \"what's\": 210,\n",
              " 'came': 211,\n",
              " 'wrong': 212,\n",
              " 'understand': 213,\n",
              " 'everyone': 214,\n",
              " 'meet': 215,\n",
              " 'right': 216,\n",
              " 'yet': 217,\n",
              " 'girl': 218,\n",
              " 'five': 219,\n",
              " 'other': 220,\n",
              " 'such': 221,\n",
              " 'three': 222,\n",
              " 'better': 223,\n",
              " 'six': 224,\n",
              " 'afraid': 225,\n",
              " 'times': 226,\n",
              " 'said': 227,\n",
              " 'world': 228,\n",
              " 'love': 229,\n",
              " \"he's\": 230,\n",
              " 'bed': 231,\n",
              " 'around': 232,\n",
              " 'likes': 233,\n",
              " 'really': 234,\n",
              " 'after': 235,\n",
              " 'already': 236,\n",
              " 'friends': 237,\n",
              " 'lost': 238,\n",
              " 'bought': 239,\n",
              " 'tried': 240,\n",
              " \"i'd\": 241,\n",
              " 'play': 242,\n",
              " 'gave': 243,\n",
              " 'little': 244,\n",
              " 'river': 245,\n",
              " \"o'clock\": 246,\n",
              " \"doesn't\": 247,\n",
              " 'able': 248,\n",
              " \"won't\": 249,\n",
              " 'took': 250,\n",
              " 'something': 251,\n",
              " 'coming': 252,\n",
              " 'buy': 253,\n",
              " 'stay': 254,\n",
              " 'study': 255,\n",
              " 'early': 256,\n",
              " 'running': 257,\n",
              " 'mine': 258,\n",
              " 'friend': 259,\n",
              " 'first': 260,\n",
              " 'married': 261,\n",
              " 'news': 262,\n",
              " 'wanted': 263,\n",
              " 'sister': 264,\n",
              " 'box': 265,\n",
              " 'caught': 266,\n",
              " 'just': 267,\n",
              " 'looked': 268,\n",
              " 'shoes': 269,\n",
              " 'while': 270,\n",
              " 'week': 271,\n",
              " 'comes': 272,\n",
              " 'station': 273,\n",
              " 'own': 274,\n",
              " 'baby': 275,\n",
              " 'ever': 276,\n",
              " 'between': 277,\n",
              " 'medicine': 278,\n",
              " 'looking': 279,\n",
              " 'usually': 280,\n",
              " 'brother': 281,\n",
              " 'smoking': 282,\n",
              " 'days': 283,\n",
              " 'fun': 284,\n",
              " 'broke': 285,\n",
              " 'cold': 286,\n",
              " 'knows': 287,\n",
              " 'drink': 288,\n",
              " 'fish': 289,\n",
              " 'dinner': 290,\n",
              " 'name': 291,\n",
              " 'eyes': 292,\n",
              " 'bad': 293,\n",
              " 'abroad': 294,\n",
              " 'small': 295,\n",
              " 'felt': 296,\n",
              " 'ask': 297,\n",
              " 'city': 298,\n",
              " 'health': 299,\n",
              " 'mind': 300,\n",
              " 'easy': 301,\n",
              " 'walk': 302,\n",
              " 'coffee': 303,\n",
              " 'difficult': 304,\n",
              " 'almost': 305,\n",
              " 'used': 306,\n",
              " 'accident': 307,\n",
              " 'london': 308,\n",
              " 'rain': 309,\n",
              " 'sit': 310,\n",
              " 'french': 311,\n",
              " 'busy': 312,\n",
              " 'town': 313,\n",
              " 'ill': 314,\n",
              " 'stop': 315,\n",
              " 'person': 316,\n",
              " 'hurry': 317,\n",
              " 'talk': 318,\n",
              " 'sure': 319,\n",
              " 'reading': 320,\n",
              " 'life': 321,\n",
              " 'poor': 322,\n",
              " 'died': 323,\n",
              " 'ran': 324,\n",
              " 'party': 325,\n",
              " 'asked': 326,\n",
              " 'happened': 327,\n",
              " 'hair': 328,\n",
              " 'told': 329,\n",
              " 'found': 330,\n",
              " 'mistakes': 331,\n",
              " 'enough': 332,\n",
              " 'forget': 333,\n",
              " 'tv': 334,\n",
              " 'wait': 335,\n",
              " 'boy': 336,\n",
              " 'large': 337,\n",
              " 'eight': 338,\n",
              " 'america': 339,\n",
              " 'afternoon': 340,\n",
              " 'others': 341,\n",
              " 'talking': 342,\n",
              " 'eat': 343,\n",
              " 'hours': 344,\n",
              " 'child': 345,\n",
              " 'parents': 346,\n",
              " 'since': 347,\n",
              " 'possible': 348,\n",
              " 'saying': 349,\n",
              " 'forgot': 350,\n",
              " 'fly': 351,\n",
              " 'fire': 352,\n",
              " 'swim': 353,\n",
              " 'flowers': 354,\n",
              " \"we're\": 355,\n",
              " 'clean': 356,\n",
              " 'music': 357,\n",
              " \"you're\": 358,\n",
              " 'heard': 359,\n",
              " 'noise': 360,\n",
              " 'crying': 361,\n",
              " 'nice': 362,\n",
              " 'boys': 363,\n",
              " 'anything': 364,\n",
              " 'which': 365,\n",
              " 'questions': 366,\n",
              " 'lives': 367,\n",
              " 'rich': 368,\n",
              " 'write': 369,\n",
              " 'language': 370,\n",
              " 'sunday': 371,\n",
              " 'became': 372,\n",
              " 'each': 373,\n",
              " 'question': 374,\n",
              " 'country': 375,\n",
              " 'king': 376,\n",
              " 'wish': 377,\n",
              " 'another': 378,\n",
              " 'bus': 379,\n",
              " 'beautiful': 380,\n",
              " 'die': 381,\n",
              " 'someone': 382,\n",
              " 'food': 383,\n",
              " 'police': 384,\n",
              " 'desk': 385,\n",
              " 'sorry': 386,\n",
              " 'things': 387,\n",
              " 'bicycle': 388,\n",
              " 'care': 389,\n",
              " 'idea': 390,\n",
              " 'slowly': 391,\n",
              " 'instead': 392,\n",
              " 'done': 393,\n",
              " 'written': 394,\n",
              " 'anxious': 395,\n",
              " \"you'll\": 396,\n",
              " 'weather': 397,\n",
              " 'hear': 398,\n",
              " 'finished': 399,\n",
              " 'behind': 400,\n",
              " 'through': 401,\n",
              " 'whether': 402,\n",
              " 'difference': 403,\n",
              " 'birds': 404,\n",
              " 'excuse': 405,\n",
              " 'move': 406,\n",
              " 'hungry': 407,\n",
              " 'nobody': 408,\n",
              " 'sick': 409,\n",
              " 'bring': 410,\n",
              " 'free': 411,\n",
              " 'son': 412,\n",
              " 'only': 413,\n",
              " 'look': 414,\n",
              " 'ready': 415,\n",
              " 'story': 416,\n",
              " \"haven't\": 417,\n",
              " 'matter': 418,\n",
              " 'along': 419,\n",
              " 'works': 420,\n",
              " 'address': 421,\n",
              " 'born': 422,\n",
              " 'later': 423,\n",
              " 'yours': 424,\n",
              " 'looks': 425,\n",
              " 'family': 426,\n",
              " \"she's\": 427,\n",
              " 'power': 428,\n",
              " 'everybody': 429,\n",
              " 'called': 430,\n",
              " 'seems': 431,\n",
              " 'kept': 432,\n",
              " 'need': 433,\n",
              " 'tokyo': 434,\n",
              " 'yourself': 435,\n",
              " 'angry': 436,\n",
              " 'waiting': 437,\n",
              " 'rumor': 438,\n",
              " 'size': 439,\n",
              " 'visit': 440,\n",
              " 'alone': 441,\n",
              " 'wherever': 442,\n",
              " 'thing': 443,\n",
              " 'business': 444,\n",
              " 'often': 445,\n",
              " 'playing': 446,\n",
              " 'makes': 447,\n",
              " 'tennis': 448,\n",
              " 'homework': 449,\n",
              " 'then': 450,\n",
              " 'wife': 451,\n",
              " 'apples': 452,\n",
              " 'learn': 453,\n",
              " 'window': 454,\n",
              " 'picture': 455,\n",
              " 'watching': 456,\n",
              " 'class': 457,\n",
              " 'interesting': 458,\n",
              " 'raining': 459,\n",
              " 'spoken': 460,\n",
              " 'china': 461,\n",
              " 'clothes': 462,\n",
              " 'telephone': 463,\n",
              " 'capital': 464,\n",
              " 'find': 465,\n",
              " 'village': 466,\n",
              " 'england': 467,\n",
              " 'longer': 468,\n",
              " 'pay': 469,\n",
              " 'dogs': 470,\n",
              " 'phone': 471,\n",
              " 'hot': 472,\n",
              " 'summer': 473,\n",
              " 'bag': 474,\n",
              " 'wash': 475,\n",
              " \"we'll\": 476,\n",
              " 'run': 477,\n",
              " 'guitar': 478,\n",
              " 'remember': 479,\n",
              " 'began': 480,\n",
              " 'cried': 481,\n",
              " 'getting': 482,\n",
              " 'hands': 483,\n",
              " 'blue': 484,\n",
              " 'sleep': 485,\n",
              " 'closed': 486,\n",
              " 'those': 487,\n",
              " 'still': 488,\n",
              " 'television': 489,\n",
              " 'worked': 490,\n",
              " 'expensive': 491,\n",
              " 'against': 492,\n",
              " 'war': 493,\n",
              " 'taking': 494,\n",
              " 'plan': 495,\n",
              " 'apple': 496,\n",
              " 'quickly': 497,\n",
              " 'dress': 498,\n",
              " 'return': 499,\n",
              " 'disappointed': 500,\n",
              " 'reap': 501,\n",
              " 'sow': 502,\n",
              " 'anyone': 503,\n",
              " 'lose': 504,\n",
              " 'japanese': 505,\n",
              " 'begins': 506,\n",
              " 'arrive': 507,\n",
              " 'easily': 508,\n",
              " 'decided': 509,\n",
              " 'sky': 510,\n",
              " 'fell': 511,\n",
              " 'studying': 512,\n",
              " 'late': 513,\n",
              " 'four': 514,\n",
              " 'sugar': 515,\n",
              " 'rains': 516,\n",
              " 'trip': 517,\n",
              " 'minutes': 518,\n",
              " 'air': 519,\n",
              " 'canada': 520,\n",
              " 'larger': 521,\n",
              " 'interested': 522,\n",
              " 'standing': 523,\n",
              " 'movie': 524,\n",
              " 'suddenly': 525,\n",
              " 'europe': 526,\n",
              " 'cup': 527,\n",
              " 'number': 528,\n",
              " 'office': 529,\n",
              " 'meeting': 530,\n",
              " 'set': 531,\n",
              " 'wants': 532,\n",
              " 'grandmother': 533,\n",
              " 'hobby': 534,\n",
              " 'population': 535,\n",
              " 'ship': 536,\n",
              " 'across': 537,\n",
              " 'kind': 538,\n",
              " 'men': 539,\n",
              " 'plane': 540,\n",
              " 'finish': 541,\n",
              " 'making': 542,\n",
              " 'whoever': 543,\n",
              " 'being': 544,\n",
              " 'mountain': 545,\n",
              " 'trouble': 546,\n",
              " 'hospital': 547,\n",
              " 'languages': 548,\n",
              " 'advice': 549,\n",
              " 'bit': 550,\n",
              " 'machine': 551,\n",
              " 'known': 552,\n",
              " 'necessary': 553,\n",
              " 'different': 554,\n",
              " 'arrested': 555,\n",
              " 'jump': 556,\n",
              " 'laughed': 557,\n",
              " 'sing': 558,\n",
              " 'stood': 559,\n",
              " 'strong': 560,\n",
              " 'cake': 561,\n",
              " 'attend': 562,\n",
              " 'fat': 563,\n",
              " 'history': 564,\n",
              " 'god': 565,\n",
              " 'over': 566,\n",
              " 'cooked': 567,\n",
              " 'black': 568,\n",
              " 'beauty': 569,\n",
              " 'talks': 570,\n",
              " \"where's\": 571,\n",
              " 'teach': 572,\n",
              " 'speaks': 573,\n",
              " 'april': 574,\n",
              " 'key': 575,\n",
              " 'age': 576,\n",
              " 'bird': 577,\n",
              " 'whose': 578,\n",
              " 'bank': 579,\n",
              " 'stolen': 580,\n",
              " 'sight': 581,\n",
              " 'truth': 582,\n",
              " 'earth': 583,\n",
              " 'hope': 584,\n",
              " 'mistake': 585,\n",
              " 'dark': 586,\n",
              " 'snow': 587,\n",
              " 'started': 588,\n",
              " 'taxi': 589,\n",
              " 'baseball': 590,\n",
              " 'shirt': 591,\n",
              " 'airport': 592,\n",
              " 'agree': 593,\n",
              " 'red': 594,\n",
              " 'broken': 595,\n",
              " 'secret': 596,\n",
              " 'eggs': 597,\n",
              " 'offer': 598,\n",
              " 'policeman': 599,\n",
              " 'horse': 600,\n",
              " 'swimming': 601,\n",
              " 'close': 602,\n",
              " 'lake': 603,\n",
              " 'knew': 604,\n",
              " 'learned': 605,\n",
              " 'empty': 606,\n",
              " 'woman': 607,\n",
              " 'prefer': 608,\n",
              " 'myself': 609,\n",
              " 'quit': 610,\n",
              " 'rather': 611,\n",
              " '3': 612,\n",
              " 'light': 613,\n",
              " \"you've\": 614,\n",
              " 'milk': 615,\n",
              " 'abandoned': 616,\n",
              " 'tends': 617,\n",
              " 'present': 618,\n",
              " 'chair': 619,\n",
              " 'show': 620,\n",
              " \"hasn't\": 621,\n",
              " 'face': 622,\n",
              " 'table': 623,\n",
              " \"shouldn't\": 624,\n",
              " 'garden': 625,\n",
              " 'novel': 626,\n",
              " 'stand': 627,\n",
              " 'explain': 628,\n",
              " 'bath': 629,\n",
              " '5': 630,\n",
              " 'cars': 631,\n",
              " 'rest': 632,\n",
              " 'dollars': 633,\n",
              " 'might': 634,\n",
              " 'm': 635,\n",
              " 'shade': 636,\n",
              " '10': 637,\n",
              " 'cats': 638,\n",
              " 'believes': 639,\n",
              " 'best': 640,\n",
              " 'arrived': 641,\n",
              " 'hardly': 642,\n",
              " 'brought': 643,\n",
              " 'famous': 644,\n",
              " 'great': 645,\n",
              " 'head': 646,\n",
              " 'promise': 647,\n",
              " 'sentences': 648,\n",
              " 'changing': 649,\n",
              " 'france': 650,\n",
              " 'dream': 651,\n",
              " 'outside': 652,\n",
              " 'speaking': 653,\n",
              " 'africa': 654,\n",
              " 'become': 655,\n",
              " 'result': 656,\n",
              " 'most': 657,\n",
              " 'strange': 658,\n",
              " 'months': 659,\n",
              " 'future': 660,\n",
              " 'students': 661,\n",
              " 'surprised': 662,\n",
              " 'sisters': 663,\n",
              " 'sun': 664,\n",
              " 'bigger': 665,\n",
              " 'hand': 666,\n",
              " 'umbrella': 667,\n",
              " 'having': 668,\n",
              " 'paris': 669,\n",
              " 'pictures': 670,\n",
              " 'older': 671,\n",
              " 'stayed': 672,\n",
              " 'doubt': 673,\n",
              " 'covered': 674,\n",
              " 'ordered': 675,\n",
              " 'whole': 676,\n",
              " 'together': 677,\n",
              " 'street': 678,\n",
              " \"there's\": 679,\n",
              " \"wouldn't\": 680,\n",
              " 'uncle': 681,\n",
              " 'amateur': 682,\n",
              " 'cricket': 683,\n",
              " 'player': 684,\n",
              " 'passed': 685,\n",
              " 'bridge': 686,\n",
              " 'leaving': 687,\n",
              " 'government': 688,\n",
              " 'pass': 689,\n",
              " 'responsible': 690,\n",
              " 'welcome': 691,\n",
              " 'bored': 692,\n",
              " 'wonderful': 693,\n",
              " 'follow': 694,\n",
              " 'shout': 695,\n",
              " 'cat': 696,\n",
              " 'map': 697,\n",
              " 'feet': 698,\n",
              " 'hat': 699,\n",
              " 'arabic': 700,\n",
              " 'husband': 701,\n",
              " 'cook': 702,\n",
              " 'kites': 703,\n",
              " 'join': 704,\n",
              " 'rice': 705,\n",
              " 'legs': 706,\n",
              " 'near': 707,\n",
              " 'walking': 708,\n",
              " 'turned': 709,\n",
              " 'listen': 710,\n",
              " 'student': 711,\n",
              " 'radio': 712,\n",
              " 'yes': 713,\n",
              " 'admit': 714,\n",
              " 'women': 715,\n",
              " 'sort': 716,\n",
              " 'tall': 717,\n",
              " 'smiled': 718,\n",
              " 'crow': 719,\n",
              " 'win': 720,\n",
              " 'wine': 721,\n",
              " 'working': 722,\n",
              " 'pulled': 723,\n",
              " 'clear': 724,\n",
              " 'trees': 725,\n",
              " 'tax': 726,\n",
              " 'worry': 727,\n",
              " 'success': 728,\n",
              " 'succeed': 729,\n",
              " 'eating': 730,\n",
              " 'kyoto': 731,\n",
              " 'meant': 732,\n",
              " 'joke': 733,\n",
              " 'glasses': 734,\n",
              " 'eleven': 735,\n",
              " 'goes': 736,\n",
              " 'accepted': 737,\n",
              " 'daughter': 738,\n",
              " 'correct': 739,\n",
              " 'eye': 740,\n",
              " 'ball': 741,\n",
              " 'lived': 742,\n",
              " 'animal': 743,\n",
              " 'fast': 744,\n",
              " 'foot': 745,\n",
              " 'fever': 746,\n",
              " 'wake': 747,\n",
              " '7': 748,\n",
              " 'nine': 749,\n",
              " 'thinking': 750,\n",
              " 'absent': 751,\n",
              " '30': 752,\n",
              " 'shall': 753,\n",
              " 'piano': 754,\n",
              " 'heart': 755,\n",
              " 'showed': 756,\n",
              " 'oil': 757,\n",
              " 'smooth': 758,\n",
              " 'plenty': 759,\n",
              " 'memory': 760,\n",
              " 'tuesday': 761,\n",
              " 'yen': 762,\n",
              " 'glass': 763,\n",
              " 'park': 764,\n",
              " 'arrogant': 765,\n",
              " 'accused': 766,\n",
              " 'important': 767,\n",
              " 'brothers': 768,\n",
              " 'under': 769,\n",
              " 'cancer': 770,\n",
              " 'half': 771,\n",
              " 'forest': 772,\n",
              " 'spanish': 773,\n",
              " 'harder': 774,\n",
              " 'rooms': 775,\n",
              " 'road': 776,\n",
              " 'satisfied': 777,\n",
              " 'neighbors': 778,\n",
              " 'plays': 779,\n",
              " 'burning': 780,\n",
              " 'death': 781,\n",
              " 'sleeping': 782,\n",
              " 'account': 783,\n",
              " 'sold': 784,\n",
              " 'travel': 785,\n",
              " 'towel': 786,\n",
              " 'add': 787,\n",
              " 'herself': 788,\n",
              " 'thank': 789,\n",
              " 'cry': 790,\n",
              " 'twenty': 791,\n",
              " 'regard': 792,\n",
              " 'protect': 793,\n",
              " 'favorite': 794,\n",
              " 'smoke': 795,\n",
              " \"aren't\": 796,\n",
              " 'officer': 797,\n",
              " 'whatever': 798,\n",
              " 'opinions': 799,\n",
              " 'says': 800,\n",
              " 'miles': 801,\n",
              " 'boss': 802,\n",
              " 'place': 803,\n",
              " 'feed': 804,\n",
              " 'word': 805,\n",
              " 'voted': 806,\n",
              " 'order': 807,\n",
              " 'bread': 808,\n",
              " 'explained': 809,\n",
              " 'rule': 810,\n",
              " 'osaka': 811,\n",
              " 'waste': 812,\n",
              " 'voice': 813,\n",
              " 'saved': 814,\n",
              " 'danger': 815,\n",
              " 'store': 816,\n",
              " 'meals': 817,\n",
              " 'words': 818,\n",
              " 'tickets': 819,\n",
              " 'speech': 820,\n",
              " 'game': 821,\n",
              " 'advise': 822,\n",
              " 'pleased': 823,\n",
              " 'allow': 824,\n",
              " 'happen': 825,\n",
              " 'pain': 826,\n",
              " 'safety': 827,\n",
              " 'italy': 828,\n",
              " 'tree': 829,\n",
              " 'fact': 830,\n",
              " 'butter': 831,\n",
              " 'advantage': 832,\n",
              " 'movies': 833,\n",
              " 'traffic': 834,\n",
              " 'mail': 835,\n",
              " 'pieces': 836,\n",
              " 'favor': 837,\n",
              " 'flying': 838,\n",
              " 'kite': 839,\n",
              " 'dangerous': 840,\n",
              " 'due': 841,\n",
              " 'illness': 842,\n",
              " 'interpret': 843,\n",
              " 'poem': 844,\n",
              " 'pick': 845,\n",
              " 'borrow': 846,\n",
              " 'subject': 847,\n",
              " 'lie': 848,\n",
              " 'helped': 849,\n",
              " 'tonight': 850,\n",
              " 'wedding': 851,\n",
              " 'fresh': 852,\n",
              " 'promote': 853,\n",
              " 'buried': 854,\n",
              " 'heat': 855,\n",
              " 'until': 856,\n",
              " 'dictionary': 857,\n",
              " 'trust': 858,\n",
              " 'rules': 859,\n",
              " 'ashamed': 860,\n",
              " 'wrote': 861,\n",
              " 'temples': 862,\n",
              " 'fifty': 863,\n",
              " 'end': 864,\n",
              " 'starts': 865,\n",
              " 'extent': 866,\n",
              " 'wind': 867,\n",
              " 'leaves': 868,\n",
              " 'computer': 869,\n",
              " 'short': 870,\n",
              " 'prize': 871,\n",
              " 'thief': 872,\n",
              " 'delhi': 873,\n",
              " 'speed': 874,\n",
              " 'breakfast': 875,\n",
              " 'case': 876,\n",
              " 'living': 877,\n",
              " 'depends': 878,\n",
              " 'teaching': 879,\n",
              " 'destroyed': 880,\n",
              " 'catch': 881,\n",
              " 'chance': 882,\n",
              " 'according': 883,\n",
              " 'front': 884,\n",
              " 'company': 885,\n",
              " 'advised': 886,\n",
              " 'college': 887,\n",
              " 'its': 888,\n",
              " 'weeks': 889,\n",
              " 'lights': 890,\n",
              " 'workers': 891,\n",
              " 'museum': 892,\n",
              " 'library': 893,\n",
              " 'answers': 894,\n",
              " 'seven': 895,\n",
              " 'test': 896,\n",
              " 'ought': 897,\n",
              " 'till': 898,\n",
              " 'couple': 899,\n",
              " 'ago': 900,\n",
              " 'except': 901,\n",
              " 'largest': 902,\n",
              " 'feeling': 903,\n",
              " 'spent': 904,\n",
              " 'teresa': 905,\n",
              " 'hello': 906,\n",
              " 'cheers': 907,\n",
              " 'ok': 908,\n",
              " 'perfect': 909,\n",
              " 'fine': 910,\n",
              " \"who's\": 911,\n",
              " 'math': 912,\n",
              " 'shot': 913,\n",
              " 'touch': 914,\n",
              " 'atheist': 915,\n",
              " 'congratulations': 916,\n",
              " 'miss': 917,\n",
              " 'reads': 918,\n",
              " 'rude': 919,\n",
              " 'lucky': 920,\n",
              " 'cafe': 921,\n",
              " 'fair': 922,\n",
              " 'fault': 923,\n",
              " 'lack': 924,\n",
              " 'top': 925,\n",
              " 'oranges': 926,\n",
              " 'sign': 927,\n",
              " 'betrayed': 928,\n",
              " 'build': 929,\n",
              " 'nests': 930,\n",
              " 'studied': 931,\n",
              " 'headache': 932,\n",
              " 'loved': 933,\n",
              " 'happiness': 934,\n",
              " 'beach': 935,\n",
              " 'throw': 936,\n",
              " 'breathed': 937,\n",
              " 'deeply': 938,\n",
              " 'simple': 939,\n",
              " 'cousin': 940,\n",
              " 'waited': 941,\n",
              " 'pretty': 942,\n",
              " 'guy': 943,\n",
              " 'enjoyed': 944,\n",
              " 'kids': 945,\n",
              " 'vase': 946,\n",
              " 'pen': 947,\n",
              " 'gym': 948,\n",
              " 'state': 949,\n",
              " 'taste': 950,\n",
              " 'continued': 951,\n",
              " 'eaten': 952,\n",
              " 'ticket': 953,\n",
              " 'green': 954,\n",
              " 'rely': 955,\n",
              " 'forgive': 956,\n",
              " 'pencil': 957,\n",
              " 'sea': 958,\n",
              " 'expect': 959,\n",
              " 'consciousness': 960,\n",
              " 'decision': 961,\n",
              " 'shut': 962,\n",
              " 'wore': 963,\n",
              " 'haunted': 964,\n",
              " 'sheets': 965,\n",
              " 'burst': 966,\n",
              " 'feels': 967,\n",
              " 'silk': 968,\n",
              " 'galileo': 969,\n",
              " 'hens': 970,\n",
              " 'bell': 971,\n",
              " 'climbed': 972,\n",
              " 'stairs': 973,\n",
              " 'sharp': 974,\n",
              " 'race': 975,\n",
              " 'boston': 976,\n",
              " 'start': 977,\n",
              " 'swollen': 978,\n",
              " 'hated': 979,\n",
              " 'stubborn': 980,\n",
              " 'nearby': 981,\n",
              " 'missed': 982,\n",
              " 'shouted': 983,\n",
              " \"what're\": 984,\n",
              " 'elevator': 985,\n",
              " 'song': 986,\n",
              " 'earns': 987,\n",
              " 'knocked': 988,\n",
              " 'robbed': 989,\n",
              " 'physics': 990,\n",
              " 'wonder': 991,\n",
              " 'stomach': 992,\n",
              " 'york': 993,\n",
              " 'wears': 994,\n",
              " 'ceased': 995,\n",
              " 'glad': 996,\n",
              " 'lion': 997,\n",
              " 'chairs': 998,\n",
              " 'passport': 999,\n",
              " 'soccer': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "metadata": {
        "id": "Jy5yt1vwS-aV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize Target language sentences"
      ]
    },
    {
      "metadata": {
        "id": "hOocCZS-S7vW",
        "colab_type": "code",
        "outputId": "cc22e8db-9419-49b6-b044-4427035510dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#Tokenizer for target language, filters should not <start> and <end>\n",
        "#remove < and > used in Target language sequences\n",
        "decoder_t = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "decoder_t.fit_on_texts(decoder_text) #Fit it on target sentences\n",
        "decoder_seq = decoder_t.texts_to_sequences(decoder_text) #Convert sentences to numbers \n",
        "decoder_seq[100:105]"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 12, 5, 1499, 2],\n",
              " [1, 28, 40, 21, 106, 3, 2],\n",
              " [1, 28, 40, 21, 208, 3, 2],\n",
              " [1, 6, 779, 19, 2],\n",
              " [1, 6, 186, 19, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "metadata": {
        "id": "JzQnUiw5TEnB",
        "colab_type": "code",
        "outputId": "d678c2e4-d637-42bd-c0c8-c3456069a2ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "#Maximum length of sentence\n",
        "max_encoder_seq_length = max([len(txt) for txt in encoder_seq])\n",
        "max_decoder_seq_length = max([len(txt) for txt in decoder_seq])\n",
        "print('Maximum sentence length for Encoder language: ', max_encoder_seq_length)\n",
        "print('Maximum sentence length for Decoder language: ', max_decoder_seq_length)\n",
        "\n",
        "#Target language Vocablury\n",
        "encoder_vocab_size = len(encoder_t.word_index)\n",
        "decoder_vocab_size = len(decoder_t.word_index)\n",
        "print('Source language vocablury size: ', encoder_vocab_size)\n",
        "print('Target language vocablury size: ', decoder_vocab_size)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sentence length for Encoder language:  22\n",
            "Maximum sentence length for Decoder language:  27\n",
            "Source language vocablury size:  2404\n",
            "Target language vocablury size:  3009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CQTdJYI0TIfg",
        "colab_type": "code",
        "outputId": "222d81e4-aba5-4e9d-ce9c-2573a58a76b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Make the encoder sequences of equal length and decoder sequence of equal length. In the decoder sequence, we are going to \n",
        "#pad the sequence in the beginning as we want the text to be at the end so that LSTM remembers it better\n",
        "#Encoder Sentences\n",
        "encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_seq, maxlen=max_encoder_seq_length, padding='pre')\n",
        "#Decoder Sentences\n",
        "decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_seq, maxlen=max_decoder_seq_length, padding='post')\n",
        "print('Source data shape: ', encoder_input_data.shape)\n",
        "print('Target data shape: ', decoder_input_data.shape)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source data shape:  (2867, 22)\n",
            "Target data shape:  (2867, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-zcnypwETMI9",
        "colab_type": "code",
        "outputId": "04a4e5de-893f-45ff-c1e4-e69c54350689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "print(encoder_input_data[0])\n",
        "decoder_input_data[0]"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0 1280]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1, 767,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "metadata": {
        "id": "wnX3h4E6TQA2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Integer to Word converter for Decoder data\n",
        "This will be used when we want to convert the output of decoder into text"
      ]
    },
    {
      "metadata": {
        "id": "s2EQhwR3TSLC",
        "colab_type": "code",
        "outputId": "61c9a1a6-76ca-479c-ebb9-843ca929a67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Pls note that the index starts from 1. Thus there will not be any text at 0 position\n",
        "int_to_word_decoder = dict((i,text) for text, i in decoder_t.word_index.items())\n",
        "int_to_word_decoder[1]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "metadata": {
        "id": "uGLfDvCkTVhy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Building Decoder Output"
      ]
    },
    {
      "metadata": {
        "id": "pN9QyRVpTYw_",
        "colab_type": "code",
        "outputId": "50a908f2-01f1-46c6-bfba-66c8d95683bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#Initialize array\n",
        "decoder_target_data = np.zeros((decoder_input_data.shape[0], decoder_input_data.shape[1]))\n",
        "\n",
        "#Shift Target output by one word\n",
        "for i in range(decoder_input_data.shape[0]):\n",
        "    for j in range(1,decoder_input_data.shape[1]):\n",
        "        decoder_target_data[i][j-1] = decoder_input_data[i][j]\n",
        "decoder_target_data[0]"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([767.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "metadata": {
        "id": "yB-WF1F4Tefw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert target data in one hot vector"
      ]
    },
    {
      "metadata": {
        "id": "CGs2eecJTf5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initialize one hot encoding array\n",
        "decoder_target_one_hot = np.zeros((decoder_input_data.shape[0], #number of sentences\n",
        "                                   decoder_input_data.shape[1], #Number of words in each sentence\n",
        "                                   len(decoder_t.word_index)+1)) #Vocab size + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhNS5JuaTk0i",
        "colab_type": "code",
        "outputId": "ed06d46d-7310-4e44-f861-62d34ead7daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Build one hot encoded array\n",
        "for i in range(decoder_target_data.shape[0]):\n",
        "    for j in range(decoder_target_data.shape[1]):\n",
        "        decoder_target_one_hot[i][j] = tf.keras.utils.to_categorical(decoder_target_data[i][j],\n",
        "                                                                     num_classes=len(decoder_t.word_index)+1) \n",
        "decoder_target_one_hot.shape"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2867, 27, 3010)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "metadata": {
        "id": "5Z5GpcIuTo17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the Training Model"
      ]
    },
    {
      "metadata": {
        "id": "1pUkvXZFTqTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define config parameters\n",
        "encoder_embedding_size = 256\n",
        "decoder_embedding_size = 256\n",
        "rnn_units = 256\n",
        "training_epochs = 25\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBt5qtB6Ttl4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Encoder"
      ]
    },
    {
      "metadata": {
        "id": "IGx36YvETwU5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvKRhfSGT2TG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input Layer\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "#Embedding layer\n",
        "encoder_embedding = tf.keras.layers.Embedding(encoder_vocab_size+1, encoder_embedding_size)\n",
        "\n",
        "#Get embedding layer output by feeding inputs\n",
        "encoder_embedding_output = encoder_embedding(encoder_inputs)\n",
        "\n",
        "#LSTM Layer and its output\n",
        "x, state_h, state_c = tf.keras.layers.LSTM(rnn_units,return_state=True)(encoder_embedding_output)\n",
        "\n",
        "#Build a list to feed Decoder\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sa4rn6KoT591",
        "colab_type": "code",
        "outputId": "352ba220-69a5-4391-c8a4-cebead67853e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "state_c"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "metadata": {
        "id": "toHYIbuwT9zD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder"
      ]
    },
    {
      "metadata": {
        "id": "F8Wj6W05T_ef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Decode input - padded Target sentences\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "#Decoder Embedding layer\n",
        "decoder_embedding = tf.keras.layers.Embedding(decoder_vocab_size + 1, decoder_embedding_size)\n",
        "\n",
        "#Embedding layer output\n",
        "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "\n",
        "#Decoder RNN\n",
        "decoder_rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "#Decoder RNN Output, State initialization from Encoder states\n",
        "#Output will be all hidden sequences, last 'h' state and last 'c' state\n",
        "x,_,_ = decoder_rnn(decoder_embedding_output, initial_state=encoder_states)\n",
        "\n",
        "#Output Layer\n",
        "decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n",
        "\n",
        "#Output of Dense layer\n",
        "decoder_outputs = decoder_dense(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4UQxLp27UFsl",
        "colab_type": "code",
        "outputId": "af3a45f8-493c-40da-891f-5ed6adfde54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/truediv:0' shape=(?, ?, 3010) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "metadata": {
        "id": "h8eR4IPiUHfG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Model using both Encoder and Decoder"
      ]
    },
    {
      "metadata": {
        "id": "KMaCo887UKd6",
        "colab_type": "code",
        "outputId": "22066b5b-a72c-429f-f5dd-f4b3be6d15e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n",
        "                              decoder_outputs) #Output of the model\n",
        "model.output"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/truediv:0' shape=(?, ?, 3010) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "metadata": {
        "id": "N9hJwpkQUWXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zvK_34EUZqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ]
    },
    {
      "metadata": {
        "id": "BHmX9Eo9UNr_",
        "colab_type": "code",
        "outputId": "0082c031-59e5-413f-fc36-68680cd464f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_one_hot, batch_size = batch_size, verbose = 1, \\\n",
        "          epochs = training_epochs, validation_split = 0.2)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2293 samples, validate on 574 samples\n",
            "Epoch 1/25\n",
            "2293/2293 [==============================] - 9s 4ms/sample - loss: 3.4702 - val_loss: 2.9528\n",
            "Epoch 2/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.6147 - val_loss: 2.9233\n",
            "Epoch 3/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.5311 - val_loss: 2.7919\n",
            "Epoch 4/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.4455 - val_loss: 2.7540\n",
            "Epoch 5/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.3939 - val_loss: 2.7001\n",
            "Epoch 6/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.3597 - val_loss: 2.6140\n",
            "Epoch 7/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.3322 - val_loss: 2.6058\n",
            "Epoch 8/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.3088 - val_loss: 2.5741\n",
            "Epoch 9/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.2859 - val_loss: 2.5738\n",
            "Epoch 10/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.2655 - val_loss: 2.5913\n",
            "Epoch 11/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.2454 - val_loss: 2.5653\n",
            "Epoch 12/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.2250 - val_loss: 2.5669\n",
            "Epoch 13/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.2056 - val_loss: 2.5964\n",
            "Epoch 14/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.1871 - val_loss: 2.6031\n",
            "Epoch 15/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.1667 - val_loss: 2.5838\n",
            "Epoch 16/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.1451 - val_loss: 2.5705\n",
            "Epoch 17/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.1232 - val_loss: 2.6110\n",
            "Epoch 18/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.1016 - val_loss: 2.5951\n",
            "Epoch 19/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.0793 - val_loss: 2.6072\n",
            "Epoch 20/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.0546 - val_loss: 2.5762\n",
            "Epoch 21/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.0295 - val_loss: 2.5864\n",
            "Epoch 22/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 1.0036 - val_loss: 2.6154\n",
            "Epoch 23/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 0.9783 - val_loss: 2.6129\n",
            "Epoch 24/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 0.9496 - val_loss: 2.5863\n",
            "Epoch 25/25\n",
            "2293/2293 [==============================] - 6s 2ms/sample - loss: 0.9218 - val_loss: 2.5819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0a2413af98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "metadata": {
        "id": "63g6uwSsoEiB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building Model for Prediction"
      ]
    },
    {
      "metadata": {
        "id": "X1NCcWN1oJc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = tf.keras.models.Model(encoder_inputs, #Padded input sequences\n",
        "                                      encoder_states) #Hidden state and Cell state at last time step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLExd1XNoO2K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the Decoder Model\n",
        "Define Input for both 'h' state and 'c' state initialization\n",
        "Get Decoder RNN outputs along with h and c state\n",
        "Get Decoder Dense layer output\n",
        "Build Model"
      ]
    },
    {
      "metadata": {
        "id": "a9TSaMt6octV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 1 - Define Input for both 'h' state and 'c' state initialization"
      ]
    },
    {
      "metadata": {
        "id": "pU-mZABmoWJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Hidden state input\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=(rnn_units,))\n",
        "\n",
        "#Cell state input\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=(rnn_units,))\n",
        "\n",
        "#Putting it together\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdX3jhhxoh3e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 2 - Get Decoder RNN outputs along with h and c state"
      ]
    },
    {
      "metadata": {
        "id": "DGROxkFxolSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get Embedding layer output\n",
        "x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "#We will use the layer which we trained earlier\n",
        "rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n",
        "\n",
        "#Why do we need this?\n",
        "decoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41a6xRudooOl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 3 - Get Decoder Dense layer output"
      ]
    },
    {
      "metadata": {
        "id": "yLMgbQ7forYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs = decoder_dense(rnn_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpHAMW6oouNO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 4 - Build Decoder Model"
      ]
    },
    {
      "metadata": {
        "id": "6oawCiAVoxSC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,  #Model inputs\n",
        "                                      [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fFDhR_-TVY-c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predicting output from Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "8iM2LoPeVWB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9258ca60-e500-45ea-f24f-917b8c7c241f"
      },
      "cell_type": "code",
      "source": [
        "decoder_t.word_index['<start>']\n",
        "int_to_word_decoder[1]"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "metadata": {
        "id": "H6X9LsMLVljV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sentence(input_sequence):\n",
        "    \n",
        "    #Get the encoder state values - Sentence embedding\n",
        "    decoder_initial_states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    #Build a sequence with '<start>' - starting sequence for Decoder\n",
        "    target_seq = np.zeros((1,1))    \n",
        "    target_seq[0][0] = decoder_t.word_index['<start>']\n",
        "    \n",
        "    #flag to check if prediction should be stopped\n",
        "    stop_loop = False\n",
        "    \n",
        "    #Initialize predicted sentence\n",
        "    predicted_sentence = ''\n",
        "    \n",
        "    num_of_predictions = 0\n",
        "    \n",
        "    #start the loop\n",
        "    while not stop_loop:\n",
        "        \n",
        "        predicted_outputs, h, c = decoder_model.predict([target_seq] + \n",
        "                                                        decoder_initial_states_value)\n",
        "        \n",
        "        #Get the predicted word index with highest probability\n",
        "        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n",
        "        \n",
        "        #Get the predicted word from predicter index\n",
        "        predicted_word = int_to_word_decoder[predicted_output]\n",
        "        \n",
        "        #Check if prediction should stop\n",
        "        if(predicted_word == '<end>' or num_of_predictions > max_decoder_seq_length):\n",
        "            \n",
        "            stop_loop = True\n",
        "            continue\n",
        "        \n",
        "        num_of_predictions += 1\n",
        "        \n",
        "        #Updated predicted sentence\n",
        "        if (len(predicted_sentence) == 0):\n",
        "            predicted_sentence = predicted_word\n",
        "        else:\n",
        "            predicted_sentence = predicted_sentence + ' ' + predicted_word\n",
        "            \n",
        "        #Update target_seq to be the predicted word index\n",
        "        target_seq[0][0] = predicted_output\n",
        "        \n",
        "        #Update initial states value for decoder\n",
        "        decoder_initial_states_value = [h,c]\n",
        "        \n",
        "    \n",
        "    return predicted_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFfbxJy8Vq8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Call Prediction function on a random sentence"
      ]
    },
    {
      "metadata": {
        "id": "kXbYYdmOVsvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "989ec206-4f95-49d4-d7cb-7b5485d4fed4"
      },
      "cell_type": "code",
      "source": [
        "#Generate a random number\n",
        "start_num = np.random.randint(0, high=len(encoder_text) - 10)\n",
        "\n",
        "#Predict model output for 5 sentences\n",
        "for i in range(start_num, start_num + 5):\n",
        "    input_seq = encoder_input_data[i : i+1]\n",
        "    predicted_sentence = decode_sentence(input_seq)\n",
        "    print('--------')\n",
        "    print ('Input sentence: ', encoder_text[i])\n",
        "    print ('Predicted sentence: ', predicted_sentence )"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------\n",
            "Input sentence:  Won't you come to dine with us?\n",
            "Predicted sentence:  तुम तुम्हें साथ नहीं नहीं गया।\n",
            "--------\n",
            "Input sentence:  You may take anything you like.\n",
            "Predicted sentence:  तुम तुम्हें नहीं नहीं हो क्या\n",
            "--------\n",
            "Input sentence:  You must be back by 10 o'clock.\n",
            "Predicted sentence:  तुम एक देर के लिए एक मदद करना होगा।\n",
            "--------\n",
            "Input sentence:  You owe me an apology for that.\n",
            "Predicted sentence:  मैं कल से मदद से चाहता हूँ।\n",
            "--------\n",
            "Input sentence:  You should have come yesterday.\n",
            "Predicted sentence:  तुम तुम क्या कर सकते हो\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whIlShE3pBAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Save encoder and decoder model"
      ]
    },
    {
      "metadata": {
        "id": "_C0vY10Fo92F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compile models to avoid error\n",
        "encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "\n",
        "#Save the models\n",
        "encoder_model.save('seq2seq_encoder_eng_hin.hd5')  #Encoder model\n",
        "decoder_model.save('seq2seq_decoder_eng_hin.hd5')  #Decoder model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZJdF32ipIBx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Save encoder and decoder tokenizers"
      ]
    },
    {
      "metadata": {
        "id": "v4d8VycNpK9R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(encoder_t,open('encoder_tokenizer_eng','wb'))\n",
        "pickle.dump(decoder_t,open('decoder_tokenizer_hin','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQ7w4oRi_--K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model with Attention"
      ]
    },
    {
      "metadata": {
        "id": "GLMBUJmeAQoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Encoder"
      ]
    },
    {
      "metadata": {
        "id": "ppTrFEA8C0PI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XKLa_N4KXhXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Input Layer\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "#Embedding layer\n",
        "encoder_embedding = tf.keras.layers.Embedding(encoder_vocab_size+1, encoder_embedding_size)\n",
        "\n",
        "#Get embedding layer output by feeding inputs\n",
        "encoder_embedding_output = encoder_embedding(encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPS1naTxAZi7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Encoder - Get all hidden states"
      ]
    },
    {
      "metadata": {
        "id": "rfUV9fBjAbfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create LSTM Layer and get All hidden states, last hidden and cell state\n",
        "encoder_lstm = tf.keras.layers.LSTM(rnn_units,return_state=True, return_sequences=True)\n",
        "\n",
        "#Get 3 outputs of LSTM Layer\n",
        "encoder_all_h_states, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
        "\n",
        "#Build a list to feed Decoder\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wV6VxohlAhTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder"
      ]
    },
    {
      "metadata": {
        "id": "gw_wkMCGAjW7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Decode input - padded Target sentences\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "#Decoder Embedding layer\n",
        "decoder_embedding = tf.keras.layers.Embedding(decoder_vocab_size + 1, decoder_embedding_size)\n",
        "\n",
        "#Embedding layer output\n",
        "decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "\n",
        "#Decoder RNN\n",
        "decoder_rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "#Decoder RNN Output, State initialization from Encoder states\n",
        "#Output will be all hidden sequences, last 'h' state and last 'c' state\n",
        "decoder_all_h_states,_,_ = decoder_rnn(decoder_embedding_output, \n",
        "                                       initial_state=encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkLMkWsWAqVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder...Alignment Matrix"
      ]
    },
    {
      "metadata": {
        "id": "EYDTtYQGAsH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1. Dot Product between Decoder_all_h_states and encoder_all_h_states\n",
        "#2. Apply softmax to get Alignment matrix\n",
        "\n",
        "#Dimensions details\n",
        "#decoder_all_states = batch_size x max_decoder_length x rnn_units\n",
        "#encoder_all_states = batch_size x max_encoder_length x rnn_units\n",
        "#score = batch_size x max_decoder_length x max_encoder_length\n",
        "#alignment matrix = batch_size x max_decoder_length x max_encoder_length\n",
        "#axes = 2 implies it will transpose and then multiply\n",
        "\n",
        "score = tf.keras.layers.dot([decoder_all_h_states, encoder_all_h_states], axes=2)\n",
        "#Now apply softmax on Score to convert each number into percentage or fraction\n",
        "alignment_matrix = tf.keras.layers.Activation('softmax')(score)\n",
        "\n",
        "#Try general and concat approaches to alignment matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMTxyqLOA1it",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder...Context Vector"
      ]
    },
    {
      "metadata": {
        "id": "Fd_rHBogA340",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Weighted sum of multiplication of Alignment matrix and encoder states\n",
        "#Dimension of context_vector =  batch_size x max_decoder_length x rnn_units\n",
        "#axes = [2,1] implies first, take the transpose of alignment_matrix and then apply dot product with encoder_all_h_states\n",
        "context_vector = tf.keras.layers.dot([alignment_matrix, encoder_all_h_states], axes=[2,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26WzyAV-A-Hg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder...Attention Vector"
      ]
    },
    {
      "metadata": {
        "id": "Nd4J2VVhA__c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Concatenate context vector and decoder_all_h_states\n",
        "#context_decoder_hidden = batch_size x max_decoder_length x rnn_units\n",
        "#attention_vector = batch_size x max_decoder_length x rnn_units(whatever was assigned in earliercell)\n",
        "\n",
        "context_decoder_hidden = tf.keras.layers.concatenate([context_vector, \n",
        "                                                      decoder_all_h_states])\n",
        "\n",
        "attention_dense_layer = tf.keras.layers.Dense(rnn_units, use_bias=False, \n",
        "                                              activation='relu')\n",
        "\n",
        "attention_vector = attention_dense_layer(context_decoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aBTQ3B3HBEsN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Decoder...Output layer"
      ]
    },
    {
      "metadata": {
        "id": "Om-srtRnBHHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Output layer. Now we build the dense layer with the probabilities equal to no of words. Each worf will be output as probability\n",
        "decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n",
        "\n",
        "#With attention input will be attention_vector and not decoder_all_h_states\n",
        "decoder_outputs = decoder_dense(attention_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMITnXwgBMNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Model using both Encoder and Decoder"
      ]
    },
    {
      "metadata": {
        "id": "J-ECdrchBOIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n",
        "                              decoder_outputs) #Output of the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz-7MzuiBWQT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ]
    },
    {
      "metadata": {
        "id": "t-yArZm1BaxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zV06eTN1BX8I",
        "colab_type": "code",
        "outputId": "d37388c8-3973-4ba9-aad7-c72f7c0da560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_one_hot, batch_size = batch_size, verbose = 1, \\\n",
        "          epochs = training_epochs, validation_split = 0.2)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2293 samples, validate on 574 samples\n",
            "Epoch 1/25\n",
            "2293/2293 [==============================] - 8s 3ms/sample - loss: 3.1082 - val_loss: 2.9887\n",
            "Epoch 2/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.6067 - val_loss: 2.8861\n",
            "Epoch 3/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.4756 - val_loss: 2.6841\n",
            "Epoch 4/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.3931 - val_loss: 2.6623\n",
            "Epoch 5/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.3441 - val_loss: 2.6679\n",
            "Epoch 6/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.3145 - val_loss: 2.6253\n",
            "Epoch 7/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.2897 - val_loss: 2.6961\n",
            "Epoch 8/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.2678 - val_loss: 2.6776\n",
            "Epoch 9/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.2490 - val_loss: 2.6929\n",
            "Epoch 10/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.2311 - val_loss: 2.7105\n",
            "Epoch 11/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.2109 - val_loss: 2.7344\n",
            "Epoch 12/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.1880 - val_loss: 2.7429\n",
            "Epoch 13/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.1646 - val_loss: 2.7332\n",
            "Epoch 14/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.1414 - val_loss: 2.7642\n",
            "Epoch 15/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.1146 - val_loss: 2.7858\n",
            "Epoch 16/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.0895 - val_loss: 2.8024\n",
            "Epoch 17/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.0585 - val_loss: 2.8228\n",
            "Epoch 18/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 1.0265 - val_loss: 2.8122\n",
            "Epoch 19/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.9905 - val_loss: 2.8592\n",
            "Epoch 20/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.9534 - val_loss: 2.9010\n",
            "Epoch 21/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.9154 - val_loss: 2.9090\n",
            "Epoch 22/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.8805 - val_loss: 2.9746\n",
            "Epoch 23/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.8398 - val_loss: 2.9905\n",
            "Epoch 24/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.7996 - val_loss: 3.0138\n",
            "Epoch 25/25\n",
            "2293/2293 [==============================] - 6s 3ms/sample - loss: 0.7582 - val_loss: 3.0392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0a1e6494a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "metadata": {
        "id": "4yORCujAB5i8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building Model for Prediction"
      ]
    },
    {
      "metadata": {
        "id": "SjpqP5SJCGbt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build the Encoder Model to predict Encoder States"
      ]
    },
    {
      "metadata": {
        "id": "7vhwQIV7B7MY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = tf.keras.models.Model(inputs=encoder_inputs, #Padded input sequences\n",
        "                                      outputs=[encoder_all_h_states] + #Hidden states at all time steps\n",
        "                                      encoder_states) #Hidden state and Cell state at last time step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_YTTJTkpxQr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the Decoder Model\n",
        "Define Input for both 'h' state and 'c' state initialization\n",
        "Define Input for all encoder states - Attention Layer\n",
        "Get Decoder RNN outputs along with h and c state\n",
        "Build Attention Layer\n",
        "Get Decoder Dense layer output using Attention vector\n",
        "Build Model"
      ]
    },
    {
      "metadata": {
        "id": "p_DD5Oogp2Qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 1 - Define Input for both 'h' state and 'c' state initialization"
      ]
    },
    {
      "metadata": {
        "id": "w2Mb8j4np1OF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hidden state input\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=(rnn_units,))\n",
        "\n",
        "#Cell state input\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=(rnn_units,))\n",
        "\n",
        "#Putting it together\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dgilwyoop6aB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 2 - Define Input encoder states - Attention Layer"
      ]
    },
    {
      "metadata": {
        "id": "jmzBttRip_gd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs = tf.keras.layers.Input(shape=(max_encoder_seq_length, rnn_units,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N6VQ2hzuqAaB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 3 - Get Decoder RNN outputs along with h and c state"
      ]
    },
    {
      "metadata": {
        "id": "JNzo5X-6qE9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get Embedding layer output\n",
        "x = decoder_embedding(decoder_inputs)\n",
        "\n",
        "#We will use the layer which we trained earlier\n",
        "rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n",
        "\n",
        "#Why do we need this?\n",
        "decoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1XjSx1woqGQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 4 - Build Attention Layer"
      ]
    },
    {
      "metadata": {
        "id": "ySmZSYcZqLkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b461a3d8-efde-421c-efc1-47a42a0649a0"
      },
      "cell_type": "code",
      "source": [
        "#Alignment score\n",
        "p_score = tf.keras.layers.dot([rnn_outputs, encoder_outputs], axes=2)\n",
        "\n",
        "#Perform softmax to get Alignment matrix\n",
        "p_alignment_matrix = tf.keras.layers.Activation('softmax')(p_score)\n",
        "\n",
        "#Context Vector\n",
        "p_context_vector = tf.keras.layers.dot([p_alignment_matrix, encoder_outputs], axes=[2,1])\n",
        "\n",
        "#Build Attention Vector\n",
        "# 1. Caoncatenate both context vector and decoder outputs\n",
        "# 2. Feed it to the Dense layer \n",
        "p_context_decoder_hidden = tf.keras.layers.concatenate([p_context_vector, rnn_outputs])\n",
        "p_attention_vector = attention_dense_layer(p_context_decoder_hidden)\n",
        "\n",
        "p_alignment_matrix"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'activation_1/truediv:0' shape=(?, ?, 22) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "metadata": {
        "id": "IL5R6CRVqSj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 5 - Get Decoder Dense layer output"
      ]
    },
    {
      "metadata": {
        "id": "Gh6nS--hqWJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs = decoder_dense(p_attention_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u8ibrrwZqYWe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 6 - Build Decoder Model"
      ]
    },
    {
      "metadata": {
        "id": "Rm-S1mY3qb32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#3 Inputs - Word, h/c state and all hidden states from encoder\n",
        "#3 Outputs - predicted word, h and c state values for next run and alignment matrix for visualization\n",
        "\n",
        "decoder_model = tf.keras.models.Model([decoder_inputs] +  #Start sequence and then word\n",
        "                                      decoder_states_inputs + #h and c state value for initialization\n",
        "                                      [encoder_outputs],  #Encoder all hidden states for Attention layer\n",
        "                                      [decoder_outputs] + #Model word prediction\n",
        "                                      decoder_states +   #h and c states for next run\n",
        "                                      [p_alignment_matrix]) #for Alignment matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mS2tjFvzqeXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Predicting output from Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "bgx6MRAoqiTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sentence(input_sequence):\n",
        "    \n",
        "    #Get the encoder state values\n",
        "    encoder_output =  encoder_model.predict(input_sequence)\n",
        "    decoder_initial_states_value = encoder_output[1:]    \n",
        "    encoded_seqs = encoder_output[0]\n",
        "    \n",
        "    #Build a sequence with '<start>' - starting sequence for Decoder\n",
        "    target_seq = np.zeros((1,1))    \n",
        "    target_seq[0][0] = decoder_t.word_index['<start>']\n",
        "    \n",
        "    #flag to check if prediction should be stopped\n",
        "    stop_loop = False\n",
        "    \n",
        "    #Initialize predicted sentence\n",
        "    predicted_sentence = ''\n",
        "    \n",
        "    #start the loop\n",
        "    while not stop_loop:\n",
        "        \n",
        "        #Decoder model with 3 inputs\n",
        "        predicted_outputs, h, c, a_matrix = decoder_model.predict([target_seq] + \n",
        "                                                                  decoder_initial_states_value +\n",
        "                                                                  [encoded_seqs])\n",
        "        \n",
        "        #Get the predicted word index with highest probability\n",
        "        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n",
        "        \n",
        "        #Get the predicted word from predicter index\n",
        "        if (predicted_output == 0):\n",
        "            predicted_word = ' '\n",
        "        else:\n",
        "            predicted_word = int_to_word_decoder[predicted_output]\n",
        "        \n",
        "        #Check if prediction should stop\n",
        "        if(predicted_word == '<end>' or len(predicted_sentence) > max_decoder_seq_length):\n",
        "            \n",
        "            stop_loop = True\n",
        "            continue\n",
        "                    \n",
        "        #Updated predicted sentence\n",
        "        if (len(predicted_sentence) == 0):\n",
        "            predicted_sentence = predicted_word\n",
        "        else:\n",
        "            predicted_sentence = predicted_sentence + ' ' + predicted_word\n",
        "            \n",
        "        #Update target_seq to be the predicted word index\n",
        "        target_seq[0][0] = predicted_output\n",
        "        \n",
        "        #Update initial states value for decoder\n",
        "        decoder_initial_states_value = [h,c]\n",
        "        \n",
        "        print(a_matrix)\n",
        "    \n",
        "    return predicted_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJkpdfWiqlf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Call Prediction function on a random sentence"
      ]
    },
    {
      "metadata": {
        "id": "-P-ujQxYqrwB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Compile encoder and decoder model"
      ]
    },
    {
      "metadata": {
        "id": "DBDZb9vGqvHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compile models to avoid error\n",
        "encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3t2ktmdhqoes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2550
        },
        "outputId": "8e56c25d-a137-4483-da21-a5d1147d2a65"
      },
      "cell_type": "code",
      "source": [
        "#Generate a random number\n",
        "start_num = np.random.randint(0, high=len(encoder_text) - 10)\n",
        "\n",
        "#Predict model output for 5 sentences\n",
        "for i in range(start_num, start_num + 5):\n",
        "    input_seq = encoder_input_data[i : i+1]\n",
        "    #print(input_seq)\n",
        "    predicted_sentence = decode_sentence(input_seq)\n",
        "    print('--------')\n",
        "    print ('Input sentence: ', encoder_text[i])\n",
        "    print ('Predicted sentence: ', predicted_sentence )"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[4.3143937e-38 1.8504775e-33 4.2454395e-27 5.9206155e-21 1.3727374e-17\n",
            "   1.6162657e-16 3.3117601e-16 4.0786500e-16 4.3975568e-16 4.5414100e-16\n",
            "   4.6065786e-16 4.6332258e-16 4.6402831e-16 4.6373984e-16 4.6298334e-16\n",
            "   4.6205351e-16 4.6110266e-16 9.9999344e-01 6.5051536e-06 5.8702924e-11\n",
            "   2.2036130e-13 5.4275768e-12]]]\n",
            "[[[3.7825784e-22 5.2033807e-19 9.2005252e-15 8.2589165e-11 7.1980821e-09\n",
            "   2.1035387e-08 2.7283047e-08 2.9322514e-08 3.0441896e-08 3.1225920e-08\n",
            "   3.1785969e-08 3.2182129e-08 3.2458551e-08 3.2649421e-08 3.2780086e-08\n",
            "   3.2869611e-08 3.2930860e-08 9.9187684e-01 8.0915336e-03 3.0116218e-05\n",
            "   6.3913768e-07 5.6867117e-07]]]\n",
            "[[[2.4118321e-15 1.4722118e-13 4.8515698e-11 1.7817207e-08 5.3245975e-07\n",
            "   1.5126996e-06 1.9275451e-06 2.0249222e-06 2.0572220e-06 2.0796742e-06\n",
            "   2.0996426e-06 2.1176002e-06 2.1332603e-06 2.1464925e-06 2.1574228e-06\n",
            "   2.1663211e-06 2.1734936e-06 9.9822813e-01 1.7105297e-03 2.6894371e-05\n",
            "   7.8566922e-07 8.5779902e-06]]]\n",
            "[[[1.0202640e-20 1.4489610e-18 1.9663453e-15 3.6774078e-12 3.3283218e-10\n",
            "   1.5442961e-09 2.2147111e-09 2.3653668e-09 2.3908717e-09 2.3991678e-09\n",
            "   2.4070468e-09 2.4159332e-09 2.4249727e-09 2.4333309e-09 2.4406286e-09\n",
            "   2.4467810e-09 2.4518498e-09 9.9987280e-01 1.2676830e-04 2.1489909e-07\n",
            "   9.0769942e-10 2.0710284e-07]]]\n",
            "[[[1.5224075e-20 6.6635339e-19 2.3146345e-16 2.0525259e-13 2.4417879e-11\n",
            "   1.8790142e-10 3.4944209e-10 4.2139026e-10 4.4973616e-10 4.6256135e-10\n",
            "   4.6926796e-10 4.7325438e-10 4.7587845e-10 4.7771642e-10 4.7905951e-10\n",
            "   4.8006199e-10 4.8082349e-10 9.9971372e-01 2.8441465e-04 3.8774991e-07\n",
            "   9.5051822e-10 1.3813373e-06]]]\n",
            "--------\n",
            "Input sentence:  I'm standing in the shade.\n",
            "Predicted sentence:  मैं अंग्रेज़ी पढ़ सकता हूँ।\n",
            "[[[0.0000000e+00 2.8105568e-36 7.8121383e-30 1.0626638e-23 2.0264012e-20\n",
            "   2.1185456e-19 4.0833078e-19 4.8737120e-19 5.1685626e-19 5.2868944e-19\n",
            "   5.3310028e-19 5.3406513e-19 5.3339320e-19 5.3195861e-19 5.3024666e-19\n",
            "   5.2852617e-19 5.2691963e-19 1.8398680e-02 6.1416189e-10 4.9549129e-07\n",
            "   9.8160088e-01 3.9725210e-09]]]\n",
            "[[[8.7631415e-27 2.5325419e-23 8.9560137e-19 1.1323300e-14 9.5950244e-13\n",
            "   2.5772511e-12 3.2664999e-12 3.5000206e-12 3.6406538e-12 3.7394072e-12\n",
            "   3.8061125e-12 3.8488921e-12 3.8747759e-12 3.8892583e-12 3.8964461e-12\n",
            "   3.8992117e-12 3.8993756e-12 4.3027545e-03 2.2386489e-06 3.7945649e-01\n",
            "   6.1623830e-01 2.3700008e-07]]]\n",
            "[[[1.27107029e-19 3.43786817e-17 5.97034968e-14 7.29099420e-11\n",
            "   3.16087334e-09 8.97972363e-09 1.14611920e-08 1.20241319e-08\n",
            "   1.21968151e-08 1.22928512e-08 1.23589849e-08 1.24051560e-08\n",
            "   1.24357875e-08 1.24545636e-08 1.24647581e-08 1.24692052e-08\n",
            "   1.24699184e-08 8.70570719e-01 9.39264864e-05 2.71448866e-02\n",
            "   1.02150269e-01 4.00312565e-05]]]\n",
            "[[[4.9809791e-21 7.6379195e-19 7.8486438e-16 9.5248115e-13 6.7537233e-11\n",
            "   2.7471708e-10 3.7901160e-10 3.9605089e-10 3.9383696e-10 3.8992926e-10\n",
            "   3.8687709e-10 3.8472694e-10 3.8320946e-10 3.8209647e-10 3.8124326e-10\n",
            "   3.8056833e-10 3.8002212e-10 9.9992037e-01 1.5694892e-05 6.7519632e-06\n",
            "   5.5635155e-05 1.5345121e-06]]]\n",
            "[[[1.2005150e-17 1.5838745e-15 1.2185446e-12 7.7475321e-10 2.3981906e-08\n",
            "   5.9919870e-08 6.9463866e-08 6.9264878e-08 6.8476602e-08 6.8159828e-08\n",
            "   6.8131882e-08 6.8224466e-08 6.8348726e-08 6.8464722e-08 6.8558670e-08\n",
            "   6.8629447e-08 6.8680656e-08 9.9881530e-01 5.3868600e-04 6.4179290e-04\n",
            "   3.1961988e-06 2.0376262e-07]]]\n",
            "[[[2.0344321e-15 2.3199693e-13 1.2850629e-10 4.0612125e-08 6.0210743e-07\n",
            "   9.9051147e-07 1.0103057e-06 9.7571865e-07 9.6050917e-07 9.5851988e-07\n",
            "   9.6179986e-07 9.6654446e-07 9.7116424e-07 9.7509178e-07 9.7821567e-07\n",
            "   9.8061435e-07 9.8241901e-07 6.0691500e-01 5.6460048e-03 3.8727522e-01\n",
            "   1.5116163e-04 2.7925552e-07]]]\n",
            "[[[2.92179448e-13 3.11899833e-12 1.11664157e-10 6.42094378e-09\n",
            "   1.04122904e-07 3.02742706e-07 4.01347478e-07 4.26513225e-07\n",
            "   4.31344972e-07 4.32209276e-07 4.32545761e-07 4.33001389e-07\n",
            "   4.33603475e-07 4.34263512e-07 4.34910902e-07 4.35504006e-07\n",
            "   4.36025971e-07 9.94973540e-01 4.91788611e-03 1.00082994e-04\n",
            "   1.70340286e-06 1.63860273e-06]]]\n",
            "--------\n",
            "Input sentence:  I'm waiting for my mother.\n",
            "Predicted sentence:  मैं कल कनाडा से देखकर काम कर\n",
            "[[[8.5587405e-31 2.7724187e-26 4.1336632e-20 3.1743359e-14 4.6089410e-11\n",
            "   4.1716711e-10 7.7081996e-10 9.2331476e-10 9.9755693e-10 1.0431441e-09\n",
            "   1.0743292e-09 1.0967059e-09 1.1131100e-09 1.1252390e-09 1.1342418e-09\n",
            "   1.1409593e-09 1.1459887e-09 5.3184625e-08 1.6706124e-02 5.7181510e-06\n",
            "   9.8328787e-01 1.7646686e-07]]]\n",
            "[[[2.0330985e-21 1.4124901e-18 1.2893464e-14 7.5030260e-11 6.2275456e-09\n",
            "   1.8775687e-08 2.4068486e-08 2.5590296e-08 2.6465644e-08 2.7203344e-08\n",
            "   2.7839212e-08 2.8375515e-08 2.8818077e-08 2.9177027e-08 2.9465150e-08\n",
            "   2.9694663e-08 2.9877370e-08 1.7856951e-06 9.9980527e-01 1.1636697e-04\n",
            "   7.4412805e-05 1.8644736e-06]]]\n",
            "[[[3.9020139e-19 5.6800454e-17 6.3707449e-14 5.2414691e-11 1.4471893e-09\n",
            "   2.8092593e-09 2.8587230e-09 2.6873921e-09 2.5887565e-09 2.5490690e-09\n",
            "   2.5394991e-09 2.5442262e-09 2.5550462e-09 2.5677189e-09 2.5801643e-09\n",
            "   2.5914835e-09 2.6013285e-09 1.0353066e-07 9.9998367e-01 1.5978861e-05\n",
            "   3.8040483e-08 1.4203830e-07]]]\n",
            "[[[3.09356373e-19 1.37944436e-17 3.72098186e-15 1.43318744e-12\n",
            "   5.31477293e-11 1.68110234e-10 2.08725523e-10 2.11545920e-10\n",
            "   2.09246898e-10 2.07825743e-10 2.07404788e-10 2.07593179e-10\n",
            "   2.08084355e-10 2.08687692e-10 2.09299592e-10 2.09868428e-10\n",
            "   2.10372206e-10 5.95698246e-09 9.99996066e-01 3.94618746e-06\n",
            "   8.37039493e-10 3.05343528e-09]]]\n",
            "--------\n",
            "Input sentence:  It is a difficult problem.\n",
            "Predicted sentence:  मुझे बहुत बड़ा है।\n",
            "[[[7.7880904e-37 8.4856366e-33 3.3294287e-27 6.9314959e-22 4.7839830e-19\n",
            "   3.3015303e-18 5.5778319e-18 6.5984934e-18 7.2311765e-18 7.7208683e-18\n",
            "   8.1152953e-18 8.4324578e-18 8.6854097e-18 8.8857503e-18 9.6947569e-16\n",
            "   7.1561540e-10 1.7090625e-06 3.2234612e-12 3.8993769e-12 3.7917612e-15\n",
            "   1.7453249e-14 9.9999833e-01]]]\n",
            "[[[3.0827911e-23 7.0994901e-21 1.2387830e-17 9.9974892e-15 1.8856540e-13\n",
            "   2.7143113e-13 2.5933059e-13 2.4738753e-13 2.4649355e-13 2.5074227e-13\n",
            "   2.5654554e-13 2.6237050e-13 2.6761064e-13 2.7208528e-13 2.5912321e-11\n",
            "   5.7505481e-06 8.9982338e-02 1.1303458e-09 1.3564145e-06 3.7521264e-08\n",
            "   1.8890722e-11 9.1001052e-01]]]\n",
            "[[[1.7154918e-21 1.5126131e-19 9.1983404e-17 5.7639169e-14 2.1661885e-12\n",
            "   5.7040891e-12 6.4327463e-12 6.1775745e-12 5.9363955e-12 5.8017332e-12\n",
            "   5.7355193e-12 5.7071795e-12 5.6987275e-12 5.7000433e-12 6.9294008e-11\n",
            "   5.1518646e-06 8.7868953e-03 5.1963081e-08 5.4930473e-07 3.3049908e-08\n",
            "   6.2465028e-10 9.9120730e-01]]]\n",
            "[[[1.6871979e-20 2.5439512e-18 1.7982247e-15 3.4367200e-13 1.4326524e-12\n",
            "   8.1374355e-13 4.9933739e-13 3.8767644e-13 3.4788565e-13 3.3346001e-13\n",
            "   3.2884982e-13 3.2824638e-13 3.2921948e-13 3.3070034e-13 8.3388747e-11\n",
            "   2.0023678e-04 9.8319823e-01 2.1724809e-11 1.0272753e-06 1.2072391e-07\n",
            "   2.4283134e-13 1.6600421e-02]]]\n",
            "[[[4.3457455e-17 3.0449996e-15 9.0832805e-13 1.4232815e-10 1.2004068e-09\n",
            "   1.3112966e-09 1.0268191e-09 8.3976137e-10 7.4342849e-10 6.9206257e-10\n",
            "   6.6279726e-10 6.4519634e-10 6.3412942e-10 6.2690542e-10 1.3075759e-08\n",
            "   3.8155077e-03 3.9320511e-01 3.4113427e-08 3.9509637e-06 7.1790635e-07\n",
            "   4.9527171e-10 6.0297459e-01]]]\n",
            "[[[9.8463283e-20 9.6418315e-18 4.6087177e-15 1.2098487e-12 1.5010455e-11\n",
            "   2.0695372e-11 1.8584962e-11 1.6598967e-11 1.5592335e-11 1.5123999e-11\n",
            "   1.4912885e-11 1.4826666e-11 1.4801011e-11 1.4803861e-11 7.3888867e-10\n",
            "   6.2742033e-03 8.9183009e-01 1.2073081e-09 6.3817771e-07 2.1323255e-07\n",
            "   8.9854209e-12 1.0189483e-01]]]\n",
            "[[[8.0039733e-18 1.8351644e-16 1.4214729e-14 9.6304377e-13 9.2159553e-12\n",
            "   1.6118239e-11 1.7180863e-11 1.6905758e-11 1.6718684e-11 1.6685009e-11\n",
            "   1.6728825e-11 1.6804041e-11 1.6887162e-11 1.6966909e-11 5.8022231e-10\n",
            "   9.5337508e-03 9.8013729e-01 2.0485860e-09 3.2922378e-07 2.8004135e-07\n",
            "   1.0725211e-11 1.0328320e-02]]]\n",
            "--------\n",
            "Input sentence:  It is easy to add 5 to 10.\n",
            "Predicted sentence:  एक बात के लिए बहुत ज़्यादा देना\n",
            "[[[2.7023120e-27 1.9273071e-23 5.7461040e-18 1.3410831e-12 1.2544522e-09\n",
            "   1.0300875e-08 1.8055301e-08 2.1235589e-08 2.2868312e-08 2.4002109e-08\n",
            "   2.4878968e-08 2.5578586e-08 2.6140356e-08 2.6590016e-08 2.6949055e-08\n",
            "   2.7234913e-08 2.7462240e-08 2.7642914e-08 9.8197961e-01 1.2549777e-03\n",
            "   1.5630141e-02 1.1349679e-03]]]\n",
            "[[[1.33428451e-22 8.46840628e-20 1.28880002e-15 3.10085464e-11\n",
            "   1.33764217e-08 1.03245235e-07 1.74699935e-07 1.97147088e-07\n",
            "   2.03756230e-07 2.06684035e-07 2.08623163e-07 2.10208086e-07\n",
            "   2.11583810e-07 2.12775234e-07 2.13793868e-07 2.14655188e-07\n",
            "   2.15375337e-07 2.15974296e-07 6.59322798e-01 1.20097641e-02\n",
            "   3.05221349e-01 2.34434903e-02]]]\n",
            "[[[6.2063219e-20 9.3604056e-18 2.0124305e-14 9.5039282e-11 2.1810070e-08\n",
            "   1.5419891e-07 2.5508641e-07 2.8371042e-07 2.8852142e-07 2.8816834e-07\n",
            "   2.8693648e-07 2.8581783e-07 2.8498820e-07 2.8442020e-07 2.8404938e-07\n",
            "   2.8381652e-07 2.8367904e-07 2.8360981e-07 2.9027870e-01 1.9274123e-02\n",
            "   6.6246527e-01 2.7978288e-02]]]\n",
            "--------\n",
            "Input sentence:  It's already nine o'clock.\n",
            "Predicted sentence:  मुश्किल हो जाओ।\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}